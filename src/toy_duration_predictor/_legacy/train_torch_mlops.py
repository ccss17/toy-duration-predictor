# --- 0. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ---
# ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— ë¨¼ì € í„°ë¯¸ë„ì—ì„œ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.
# pip install torch numpy pytorch-lightning wandb optuna gradio

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset, random_split
import pytorch_lightning as pl
from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
import numpy as np
import optuna
import gradio as gr
import os

# --- 1. ê°€ìƒ ë°ì´í„° ìƒì„± ë° PyTorch Lightning ë°ì´í„° ëª¨ë“ˆ (ì—…ë°ì´íŠ¸) ---
# ì‹¤ì œë¡œëŠ” ì´ ë¶€ë¶„ì— MIDI ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³  ë¡œë“œí•˜ëŠ” ì½”ë“œê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤.

# ë°ì´í„° ê´€ë ¨ ìƒìˆ˜
MAX_SEQ_LENGTH = 32
NUM_SINGERS = 100
NUM_SAMPLES = 100000  # ì „ì²´ ìƒ˜í”Œ ìˆ˜ (10ë§Œê°œë¡œ ì¦ê°€)
BATCH_SIZE = 256      # ë°°ì¹˜ í¬ê¸°

class DurationDataModule(pl.LightningDataModule):
    def __init__(self, batch_size):
        super().__init__()
        self.batch_size = batch_size
        self.full_dataset = None
        self.train_dataset = None
        self.val_dataset = None
        self.test_dataset = None

    def prepare_data(self):
        # ì´ ë©”ì†Œë“œëŠ” ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.
        # ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê±°ë‚˜ ìƒì„±í•˜ëŠ” ë¡œì§ì„ ì—¬ê¸°ì— ë„£ìŠµë‹ˆë‹¤.
        pass

    def setup(self, stage=None):
        # ëª¨ë“  GPU/TPUì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ë¶„í• í•˜ê³  í• ë‹¹í•©ë‹ˆë‹¤.
        if not self.full_dataset:
            # ê°€ìƒì˜ ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±
            durations = torch.rand(NUM_SAMPLES, MAX_SEQ_LENGTH)
            sids = torch.randint(0, NUM_SINGERS, (NUM_SAMPLES, MAX_SEQ_LENGTH))
            labels = durations * torch.rand_like(durations) * 2
            self.full_dataset = TensorDataset(durations, sids, labels)

        # í›ˆë ¨(80%), ê²€ì¦(10%), í…ŒìŠ¤íŠ¸(10%) ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„í• 
        train_size = int(0.8 * len(self.full_dataset))
        val_size = int(0.1 * len(self.full_dataset))
        test_size = len(self.full_dataset) - train_size - val_size
        
        # random_splitì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë‚˜ëˆ” (ë§¤ë²ˆ ë™ì¼í•œ ë¶„í• ì„ ìœ„í•´ ì‹œë“œ ê³ ì •)
        self.train_dataset, self.val_dataset, self.test_dataset = random_split(
            self.full_dataset, [train_size, val_size, test_size],
            generator=torch.Generator().manual_seed(42)
        )

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=os.cpu_count()//2)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=os.cpu_count()//2)
        
    def test_dataloader(self):
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ì¶”ê°€
        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=os.cpu_count()//2)

# --- 2. PyTorch Lightning ëª¨ë¸ (ì—…ë°ì´íŠ¸) ---
# í…ŒìŠ¤íŠ¸ ìŠ¤í… ì¶”ê°€

class DurationPredictor(pl.LightningModule):
    def __init__(self, hparams):
        super().__init__()
        self.save_hyperparameters(hparams)
        self.sid_embedding = nn.Embedding(self.hparams.num_singers, self.hparams.sid_embedding_dim)
        gru_input_dim = 1 + self.hparams.sid_embedding_dim
        self.gru = nn.GRU(gru_input_dim, self.hparams.gru_units, self.hparams.num_gru_layers, 
                          batch_first=True, bidirectional=True, 
                          dropout=self.hparams.dropout_rate if self.hparams.num_gru_layers > 1 else 0)
        self.fc_out = nn.Linear(self.hparams.gru_units * 2, 1)
        self.loss_fn = nn.MSELoss()

    def forward(self, duration_input, sid_input):
        sid_embedded = self.sid_embedding(sid_input)
        duration_reshaped = duration_input.unsqueeze(-1)
        x = torch.cat([duration_reshaped, sid_embedded], dim=-1)
        gru_output, _ = self.gru(x)
        predictions = self.fc_out(gru_output)
        return predictions

    def _common_step(self, batch, batch_idx):
        durations, sids, labels = batch
        predictions = self.forward(durations, sids)
        loss = self.loss_fn(predictions.squeeze(-1), labels)
        return loss

    def training_step(self, batch, batch_idx):
        loss = self._common_step(batch, batch_idx)
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx):
        loss = self._common_step(batch, batch_idx)
        self.log('val_loss', loss, on_epoch=True, prog_bar=True)
        return loss
    
    def test_step(self, batch, batch_idx):
        loss = self._common_step(batch, batch_idx)
        self.log('test_loss', loss, on_epoch=True, prog_bar=True)
        return loss

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)

# --- 3. Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ---
def objective(trial: optuna.Trial):
    hparams = {
        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
        'sid_embedding_dim': trial.suggest_categorical('sid_embedding_dim', [8, 16, 32]),
        'gru_units': trial.suggest_categorical('gru_units', [64, 128]),
        'num_gru_layers': trial.suggest_int('num_gru_layers', 1, 2),
        'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.4),
        'num_singers': NUM_SINGERS
    }
    
    wandb_logger = WandbLogger(project="duration_predictor_optuna", name=f"trial-{trial.number}", group="optuna-study")
    wandb_logger.log_hyperparams(hparams)

    model = DurationPredictor(hparams)
    datamodule = DurationDataModule(batch_size=BATCH_SIZE)

    trainer = pl.Trainer(
        logger=wandb_logger, max_epochs=5, accelerator="auto", devices=1,
        enable_checkpointing=False, callbacks=[EarlyStopping(monitor="val_loss", mode="min", patience=2)]
    )
    trainer.fit(model, datamodule)
    return trainer.callback_metrics["val_loss"].item()

# --- 4. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ (ì—…ë°ì´íŠ¸) ---
if __name__ == '__main__':
    # --- 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ---
    print("--- 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ (Optuna) ---")
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=10) # ì‹¤ì œë¡œëŠ” 50~100íšŒ ì´ìƒ ê¶Œì¥

    print("ìµœì í™” ì™„ë£Œ!")
    best_hparams = study.best_params
    print(f"ìµœê³ ì˜ val_loss: {study.best_value}\nìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_hparams}")
    
    # --- 2. ìµœì  ëª¨ë¸ í›ˆë ¨ ë° ì €ì¥ ---
    print("\n--- 2. ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì‹œì‘ ---")
    final_hparams = best_hparams
    final_hparams['num_singers'] = NUM_SINGERS
    
    datamodule = DurationDataModule(batch_size=BATCH_SIZE)
    model = DurationPredictor(final_hparams)
    
    wandb_logger = WandbLogger(project="duration_predictor_final", name="final_best_model")
    wandb_logger.log_hyperparams(final_hparams)

    checkpoint_callback = ModelCheckpoint(
        dirpath='checkpoints', filename='best-model', save_top_k=1, monitor='val_loss', mode='min'
    )
    
    trainer = pl.Trainer(
        logger=wandb_logger, max_epochs=20, accelerator="auto", devices=1,
        callbacks=[checkpoint_callback, EarlyStopping(monitor="val_loss", mode="min", patience=4)]
    )
    trainer.fit(model, datamodule)
    print("ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    
    # --- 3. ìµœì¢… ì„±ëŠ¥ í‰ê°€ (í…ŒìŠ¤íŠ¸) ---
    print(f"ì €ì¥ëœ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ê²½ë¡œ: {checkpoint_callback.best_model_path}")
    # trainer.test()ëŠ” ìµœê³ ì˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ í‰ê°€ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
    test_results = trainer.test(ckpt_path='best', datamodule=datamodule)
    print("ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼:", test_results)
    
    # --- 4. Gradio ë°ëª¨ ì‹¤í–‰ ---
    print("\n--- 4. Gradio ë°ëª¨ ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰ ---")
    
    best_model = DurationPredictor.load_from_checkpoint(checkpoint_callback.best_model_path)
    best_model.eval()

    def predict_duration(singer_id_str, duration_sequence_str):
        try:
            singer_id = int(singer_id_str)
            durations = [float(d.strip()) for d in duration_sequence_str.split(',')]
            
            if len(durations) > MAX_SEQ_LENGTH:
                durations = durations[:MAX_SEQ_LENGTH]
            else:
                durations += [0] * (MAX_SEQ_LENGTH - len(durations))

            duration_tensor = torch.tensor(durations, dtype=torch.float32).unsqueeze(0)
            sid_tensor = torch.full_like(duration_tensor, singer_id, dtype=torch.long)

            with torch.no_grad():
                prediction = best_model(duration_tensor, sid_tensor)
            
            output_sequence = prediction.squeeze().tolist()
            return ", ".join([f"{x:.4f}" for x in output_sequence])

        except Exception as e:
            return f"ì˜¤ë¥˜ ë°œìƒ: {e}"

    iface = gr.Interface(
        fn=predict_duration,
        inputs=[
            gr.Textbox(label="ê°€ìˆ˜ ID (Singer ID)", value="10"),
            gr.Textbox(label="ìŒí‘œ ê¸¸ì´ ì‹œí€€ìŠ¤ (ì‰¼í‘œë¡œ êµ¬ë¶„)", 
                       value="0.1, 0.2, 0.15, 0.5, 0.4, 0.12, 0.1, 0.25")
        ],
        outputs=gr.Textbox(label="ì˜ˆì¸¡ëœ ìŒí‘œ ê¸¸ì´ ì‹œí€€ìŠ¤"),
        title="ğŸµ Duration Predictor (ë¦¬ë“¬ í‘œí˜„ ì˜ˆì¸¡ê¸°)",
        description="ê°€ìˆ˜ IDì™€ ì •ê·œ ìŒí‘œ ê¸¸ì´ ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥í•˜ë©´, í•´ë‹¹ ê°€ìˆ˜ì˜ ê³ ìœ í•œ ë¦¬ë“¬ í‘œí˜„ì´ ì ìš©ëœ ìŒí‘œ ê¸¸ì´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤."
    )
    
    iface.launch()
