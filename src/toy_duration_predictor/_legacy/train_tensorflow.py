# --- 0. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ---
# ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— ë¨¼ì € í„°ë¯¸ë„ì—ì„œ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.
# pip install tensorflow numpy wandb keras-tuner gradio

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model
import numpy as np
import keras_tuner as kt
from wandb.keras import WandbCallback
import gradio as gr
import os

# --- 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ìƒìˆ˜ ì •ì˜ ---
MAX_SEQ_LENGTH = 32
NUM_SINGERS = 100
NUM_SAMPLES = 100000
BATCH_SIZE = 256
BUFFER_SIZE = 10000 # tf.data.Dataset ì…”í”Œì„ ìœ„í•œ ë²„í¼ í¬ê¸°

# --- 2. ë°ì´í„° ì¤€ë¹„ (tf.data.Dataset ì‚¬ìš©) ---
print("--- ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘... ---")

def generate_dummy_data():
    """ê°€ìƒì˜ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ëŠ” ì œë„ˆë ˆì´í„° í•¨ìˆ˜"""
    for _ in range(NUM_SAMPLES):
        duration = np.random.rand(MAX_SEQ_LENGTH).astype(np.float32)
        sid = np.random.randint(0, NUM_SINGERS, (MAX_SEQ_LENGTH,)).astype(np.int32)
        label = (duration * np.random.rand(MAX_SEQ_LENGTH) * 2).astype(np.float32)
        # Keras ëª¨ë¸ì€ ì…ë ¥ê³¼ ì¶œë ¥ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°›ëŠ” ê²ƒì´ í¸ë¦¬í•©ë‹ˆë‹¤.
        yield {'duration_input': duration, 'sid_input': sid}, label

# tf.data.Dataset ê°ì²´ ìƒì„±
full_dataset = tf.data.Dataset.from_generator(
    generate_dummy_data,
    output_signature=(
        {'duration_input': tf.TensorSpec(shape=(MAX_SEQ_LENGTH,), dtype=tf.float32),
         'sid_input': tf.TensorSpec(shape=(MAX_SEQ_LENGTH,), dtype=tf.int32)},
        tf.TensorSpec(shape=(MAX_SEQ_LENGTH,), dtype=tf.float32)
    )
)

# í›ˆë ¨(80%), ê²€ì¦(10%), í…ŒìŠ¤íŠ¸(10%) ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„í• 
full_dataset = full_dataset.shuffle(BUFFER_SIZE, seed=42) # ë¶„í•  ì „ ì „ì²´ ì…”í”Œ
train_size = int(0.8 * NUM_SAMPLES)
val_size = int(0.1 * NUM_SAMPLES)

train_dataset = full_dataset.take(train_size)
val_and_test_dataset = full_dataset.skip(train_size)
val_dataset = val_and_test_dataset.take(val_size)
test_dataset = val_and_test_dataset.skip(val_size)

# ë°ì´í„°ë¡œë” ìƒì„± (ë°°ì¹˜, í”„ë¦¬í˜ì¹˜ ë“± ìµœì í™”)
train_loader = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
val_loader = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
test_loader = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

print(f"í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {train_size}")
print(f"ê²€ì¦ ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {val_size}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {NUM_SAMPLES - train_size - val_size}")

# --- 3. KerasTunerë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ---

def build_model(hp: kt.HyperParameters):
    """KerasTunerê°€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íƒìƒ‰í•˜ê¸° ìœ„í•œ ëª¨ë¸ ë¹Œë“œ í•¨ìˆ˜"""
    
    # ì…ë ¥ì¸µ ì •ì˜
    duration_input = layers.Input(shape=(MAX_SEQ_LENGTH,), name='duration_input')
    sid_input = layers.Input(shape=(MAX_SEQ_LENGTH,), name='sid_input')

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ê³µê°„ ì •ì˜
    sid_embedding_dim = hp.Choice('sid_embedding_dim', values=[8, 16, 32])
    gru_units = hp.Choice('gru_units', values=[64, 128])
    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')
    
    # ëª¨ë¸ ë ˆì´ì–´
    sid_embedding = layers.Embedding(input_dim=NUM_SINGERS, output_dim=sid_embedding_dim)(sid_input)
    duration_reshaped = layers.Reshape((MAX_SEQ_LENGTH, 1))(duration_input)
    
    x = layers.Concatenate()([duration_reshaped, sid_embedding])
    
    # Kerasì˜ GRUëŠ” ê¸°ë³¸ì ìœ¼ë¡œ dropout ì¸ìë¥¼ ê°€ì§
    x = layers.Bidirectional(layers.GRU(gru_units, return_sequences=True))(x)
    x = layers.Bidirectional(layers.GRU(gru_units, return_sequences=True))(x)
    
    outputs = layers.TimeDistributed(layers.Dense(1, activation='linear'))(x)

    model = Model(inputs=[duration_input, sid_input], outputs=outputs)
    
    model.compile(optimizer=keras.optimizers.Adam(learning_rate), loss='mean_squared_error')
    
    return model

print("\n--- 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ (KerasTuner) ---")
tuner = kt.Hyperband(
    build_model,
    objective='val_loss',
    max_epochs=10,
    factor=3,
    directory='keras_tuner_dir',
    project_name='duration_predictor'
)

# KerasTuner ì‹¤í–‰
tuner.search(train_loader, epochs=10, validation_data=val_loader, callbacks=[keras.callbacks.EarlyStopping(patience=3)])

# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì¶œ
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
print("ìµœì í™” ì™„ë£Œ!")
print(f"ìµœì ì˜ í•™ìŠµë¥ : {best_hps.get('learning_rate')}")
print(f"ìµœì ì˜ ì„ë² ë”© ì°¨ì›: {best_hps.get('sid_embedding_dim')}")
print(f"ìµœì ì˜ GRU ìœ ë‹› ìˆ˜: {best_hps.get('gru_units')}")

# --- 4. ìµœì¢… ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ ---
print("\n--- 2. ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì‹œì‘ ---")

# WandB ì´ˆê¸°í™”
import wandb
wandb.init(project="duration_predictor_tf_keras", config=best_hps.values)

# ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ ë¹Œë“œ
final_model = tuner.hypermodel.build(best_hps)

# ì²´í¬í¬ì¸íŠ¸ ë° ì¡°ê¸° ì¢…ë£Œ ì½œë°± ì„¤ì •
checkpoint_cb = keras.callbacks.ModelCheckpoint("best_model.keras", save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)

# ëª¨ë¸ í›ˆë ¨
final_model.fit(
    train_loader,
    epochs=50,
    validation_data=val_loader,
    callbacks=[WandbCallback(), checkpoint_cb, early_stopping_cb]
)
print("ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

# --- 5. ìµœì¢… ì„±ëŠ¥ í‰ê°€ (í…ŒìŠ¤íŠ¸ì…‹) ---
print("\n--- 3. ìµœì¢… ëª¨ë¸ í‰ê°€ ì‹œì‘ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì‚¬ìš©) ---")
best_model = keras.models.load_model("best_model.keras")
test_loss = best_model.evaluate(test_loader)
print("-" * 50)
print(f"ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (MSE): {test_loss:.6f}")
wandb.log({"test_loss": test_loss})
wandb.finish()

# --- 6. Gradio ë°ëª¨ ì‹¤í–‰ ---
print("\n--- 4. Gradio ë°ëª¨ ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰ ---")

def predict_duration_keras(singer_id_str, duration_sequence_str):
    try:
        singer_id = int(singer_id_str)
        durations = [float(d.strip()) for d in duration_sequence_str.split(',')]
        
        if len(durations) > MAX_SEQ_LENGTH:
            durations = durations[:MAX_SEQ_LENGTH]
        else:
            durations += [0] * (MAX_SEQ_LENGTH - len(durations))

        # Keras ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜ (Numpy ë°°ì—´)
        duration_np = np.array(durations, dtype=np.float32).reshape(1, -1)
        sid_np = np.full_like(duration_np, singer_id, dtype=np.int32)

        # ì˜ˆì¸¡ ì‹¤í–‰
        prediction = best_model.predict({'duration_input': duration_np, 'sid_input': sid_np})
        
        output_sequence = prediction.flatten().tolist()
        return ", ".join([f"{x:.4f}" for x in output_sequence])
    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {e}"

iface = gr.Interface(
    fn=predict_duration_keras,
    inputs=[
        gr.Textbox(label="ê°€ìˆ˜ ID (Singer ID)", value="10"),
        gr.Textbox(label="ìŒí‘œ ê¸¸ì´ ì‹œí€€ìŠ¤ (ì‰¼í‘œë¡œ êµ¬ë¶„)", 
                   value="0.1, 0.2, 0.15, 0.5, 0.4, 0.12, 0.1, 0.25")
    ],
    outputs=gr.Textbox(label="ì˜ˆì¸¡ëœ ìŒí‘œ ê¸¸ì´ ì‹œí€€ìŠ¤"),
    title="ğŸµ Duration Predictor (Keras + MLOps)",
    description="Kerasë¡œ í›ˆë ¨ëœ ëª¨ë¸ì…ë‹ˆë‹¤. ê°€ìˆ˜ IDì™€ ì •ê·œ ìŒí‘œ ê¸¸ì´ ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥í•˜ë©´, í•´ë‹¹ ê°€ìˆ˜ì˜ ê³ ìœ í•œ ë¦¬ë“¬ í‘œí˜„ì´ ì ìš©ëœ ìŒí‘œ ê¸¸ì´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤."
)

iface.launch()
